[
  {
    "objectID": "sketching_gestalt_space.html",
    "href": "sketching_gestalt_space.html",
    "title": "Sketching in Gestalt Space",
    "section": "",
    "text": "The paper describes a method for creating abstracted geometry based on an initial pre-segmented input model. This is transformed into an abstracted version by applying Gestalt principles on objects according to sketches by the user. Groupings of the scene elements according to these rules are computed and abstractions are created summarizing the objects into bounding volumes or replacing geometries with scaled versions, depending on the intent of the user abstraction\n\n\nDetermining potentials for Abstraction in the scene, based on the Gestalt principles is the most Interesting part of this method.\n\nGenerate scene including a set of clear patterns (grid, dense cluster, line, tight grid, isolated cubes)\nCompute groups based on Gestalt principles (proximity, regularity, continuity, symmetry)",
    "crumbs": [
      "Sketching in Gestalt Space"
    ]
  },
  {
    "objectID": "sketching_gestalt_space.html#overview",
    "href": "sketching_gestalt_space.html#overview",
    "title": "Sketching in Gestalt Space",
    "section": "",
    "text": "The paper describes a method for creating abstracted geometry based on an initial pre-segmented input model. This is transformed into an abstracted version by applying Gestalt principles on objects according to sketches by the user. Groupings of the scene elements according to these rules are computed and abstractions are created summarizing the objects into bounding volumes or replacing geometries with scaled versions, depending on the intent of the user abstraction\n\n\nDetermining potentials for Abstraction in the scene, based on the Gestalt principles is the most Interesting part of this method.\n\nGenerate scene including a set of clear patterns (grid, dense cluster, line, tight grid, isolated cubes)\nCompute groups based on Gestalt principles (proximity, regularity, continuity, symmetry)",
    "crumbs": [
      "Sketching in Gestalt Space"
    ]
  },
  {
    "objectID": "sketching_gestalt_space.html#implementation",
    "href": "sketching_gestalt_space.html#implementation",
    "title": "Sketching in Gestalt Space",
    "section": "Implementation",
    "text": "Implementation\nIn this notebook I implement a demonstration of the core principles of using gestalt rules to guiding the abstraction of a scene.\nFor this we load / generate a simple scene with primitives in it ordered and aligned in a way so that different patterns and Gestalt shapes emerge naturally to the viewer.\nBased on this the program implements metrics from the paper to programmatically figure out these structures and mark the objects accordingly.\nThe program then visualizes this intial guess and lets the user show his intent of which of the objects to abstract.\nNow using the intent and the previous knowledge a simple abstraction is computed and also rendered in the scene\nThis interactive loop now should already provide a basic understanding of how the method from the paper works albeit being very simple in its design.\n\nDetermining Orientation and Shape Dimensions using PCA\n\n# Set random seed for reproducibility\n# random.seed(42)\n# np.random.seed(42)\n\n\n# # | export\n# # Generate a simple scene with cube primitives in structured arrangement\n# def generate_cube_grid(n_x=4, n_y=3, spacing=1.5):\n#     cube = o3d.geometry.TriangleMesh.create_box(width=1.0, height=1.0, depth=1.0)\n#     cube.compute_vertex_normals()\n#     scene = []\n#     for i in range(n_x):\n#         for j in range(n_y):\n#             new_cube = copy.deepcopy(cube)\n#             new_cube.translate(np.array([i * spacing, j * spacing, 0]))\n#             scene.append(new_cube)\n#     return scene\n\n\n# # | export\n# def generate_custom_scene():\n#     cube = o3d.geometry.TriangleMesh.create_box(width=1.0, height=1.0, depth=1.0)\n#     cube.compute_vertex_normals()\n#     scene = []\n\n#     def place_grid(start, nx, ny, dx=1.5, dy=1.5):\n#         for i in range(nx):\n#             for j in range(ny):\n#                 c = copy.deepcopy(cube)\n#                 c.translate([start[0] + i * dx, start[1] + j * dy, start[2]])\n#                 scene.append(c)\n\n#     def place_line(start, count, d=1.5):\n#         for i in range(count):\n#             c = copy.deepcopy(cube)\n#             c.translate([start[0] + i * d, start[1], start[2]])\n#             scene.append(c)\n\n#     def place_cluster(center, count=6, spread=0.6):\n#         for _ in range(count):\n#             c = copy.deepcopy(cube)\n#             offset = np.random.normal(scale=spread, size=3)\n#             c.translate(center + offset)\n#             scene.append(c)\n\n#     # Pattern 1: Structured Grid → suggests regularity and proximity\n#     place_grid(start=np.array([0, 0, 0]), nx=3, ny=2)\n\n#     # Pattern 2: Line → suggests continuity\n#     place_line(start=np.array([7, 0, 0]), count=5)\n\n#     # Pattern 3: Dense Cluster → suggests proximity, but low regularity\n#     place_cluster(center=np.array([0, 6, 0]), count=7)\n\n#     # Pattern 4: Small tight grid → proximity + potential symmetry\n#     place_grid(start=np.array([7, 6, 0]), nx=2, ny=2, dx=1.0, dy=1.0)\n\n#     # Pattern 5: Short isolated line → separate group\n#     place_line(start=np.array([3, 3, 0]), count=3)\n\n#     return scene\n\n\n# # | export\n# # Create axis-aligned bounding box for a group of cubes\n# # color them based on their regularity scores\n# def create_group_aabb(group, regularity_score):\n#     all_points = np.vstack([np.asarray(cube.get_axis_aligned_bounding_box().get_box_points()) for cube in group])\n#     aabb = o3d.geometry.AxisAlignedBoundingBox.create_from_points(o3d.utility.Vector3dVector(all_points))\n\n#     # Color code based on score\n#     if regularity_score &gt;= 0.8:\n#         aabb.color = (0.0, 1.0, 0.0)  # Green = regular\n#     elif regularity_score &gt;= 0.5:\n#         aabb.color = (1.0, 0.65, 0.0)  # Orange = moderate\n#     else:\n#         aabb.color = (1.0, 0.0, 0.0)  # Red = irregular\n#     return aabb\n\n\n\nProximity Metric\nThe proximity metric groups objects that are close to one another into a single group\n\n# # | export\n# # Grouping by proximity (Euclidean distance between centers)\n# def group_by_proximity(cubes, threshold=2.0):\n#     centers = [cube.get_center() for cube in cubes]\n#     groups = []\n#     used = set()\n#     for i in range(len(cubes)):\n#         if i in used:\n#             continue\n#         group = [cubes[i]]\n#         used.add(i)\n#         for j in range(i + 1, len(cubes)):\n#             if j not in used and np.linalg.norm(centers[i] - centers[j]) &lt; threshold:\n#                 group.append(cubes[j])\n#                 used.add(j)\n#         groups.append(group)\n#     return groups\n\n\n\nRegularity Scoring\nAs a second metric in combination with proximity i include a regularity score for each group.\nColor bounding boxes based on that score: * 🟢 green = high regularity (≥ 0.8) * 🟠 orange = medium regularity (0.5–0.8) * 🔴 red = low regularity (&lt; 0.5)\n\n# # | export\n# def compute_group_regularity(group):\n#     if len(group) &lt; 3:\n#         return 0.0  # too small to assess regularity\n\n#     centers = np.array([cube.get_center() for cube in group])\n#     diffs = []\n\n#     for i in range(len(centers)):\n#         for j in range(i + 1, len(centers)):\n#             diff = centers[j] - centers[i]\n#             if np.linalg.norm(diff) &gt; 1e-6:\n#                 diffs.append(diff)\n\n#     if len(diffs) &lt; 3:\n#         return 0.0  # not enough meaningful direction vectors\n\n#     diffs = np.array(diffs)\n#     norm_diffs = np.linalg.norm(diffs, axis=1, keepdims=True)\n#     unit_dirs = diffs / norm_diffs\n\n#     # PCA via covariance matrix of unit directions\n#     try:\n#         cov = np.cov(unit_dirs.T)\n#         eigenvalues, _ = np.linalg.eigh(cov)\n#         principal_val = eigenvalues[-1]  # largest eigenvalue = dominant direction\n#         return float(np.clip(principal_val, 0.0, 1.0))  # clip to avoid NaNs\n#     except np.linalg.LinAlgError:\n#         return 0.0\n\n\n# Run demo\nscene = generate_custom_scene()\ngroups = group_by_proximity(scene, threshold=2.2)\n\nbounding_boxes = []\nfor i, group in enumerate(groups):\n    reg = compute_group_regularity(group)\n    print(f\"Group {i}: Size={len(group)}, Regularity={reg:.2f}\")\n    aabb = create_group_aabb(group, reg)\n    bounding_boxes.append(aabb)\n\nGroup 0: Size=4, Regularity=0.60\nGroup 1: Size=2, Regularity=0.00\nGroup 2: Size=2, Regularity=0.00\nGroup 3: Size=2, Regularity=0.00\nGroup 4: Size=1, Regularity=0.00\nGroup 5: Size=7, Regularity=0.46\nGroup 6: Size=4, Regularity=0.60\nGroup 7: Size=2, Regularity=0.00\nGroup 8: Size=1, Regularity=0.00\n\n\n\n# Visualize\no3d.visualization.draw_geometries(scene + bounding_boxes)\n\n[Open3D WARNING] GLFW Error: Wayland: The platform does not support setting the window position\n[Open3D WARNING] Failed to initialize GLEW.\n[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n# from abc import ABC, abstractmethod\n# from typing import Tuple\n\n\n# class Evaluator(ABC):\n#     \"\"\"\n#     Any grouping method must override `evaluate`.\n#     It should return:\n#       - groups:   list[list[o3d.TriangleMesh]]\n#       - scores:   list[float]  (one per group, 0-1, or None if not meaningful)\n#     \"\"\"\n#     name: str            # short label used in menus\n#     color: Tuple[float]  # default RGB for this method’s boxes\n\n#     @abstractmethod\n#     def evaluate(self, cubes) -&gt; tuple[list, list]:\n#         ...\n\n\n# class ProximityEval(Evaluator):\n#     name, color = \"Proximity\", (0.2, 0.6, 1.0)  # light-blue\n\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)\n#         # No per-group quality score → fill with None\n#         return groups, [None]*len(groups)\n\n\n# class RegularityEval(Evaluator):\n#     name, color = \"Regularity\", (0.0, 0.8, 0.2)  # green\n\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)  # reuse\n#         scores = [compute_group_regularity(g) for g in groups]\n#         return groups, scores\n\n\n# def groups_to_bboxes(groups, scores, base_color):\n#     boxes = []\n#     for g, s in zip(groups, scores):\n#         aabb = create_group_aabb(g, s if s is not None else 1.0)\n#         # over-write color so each evaluator keeps its own palette\n#         aabb.color = base_color\n#         boxes.append(aabb)\n#     return boxes\n\n\n# evaluators = [ProximityEval(), RegularityEval()]\n# state = {\"idx\": 0, \"scene\": scene}  # mutable dict so the lambda sees updates\n\n# def refresh(vis):\n#     vis.clear_geometries()\n#     vis.add_geometry(*state[\"scene\"])             # raw cubes\n#     ev = evaluators[state[\"idx\"]]\n#     groups, scores = ev.evaluate(state[\"scene\"])\n#     for box in groups_to_bboxes(groups, scores, ev.color):\n#         vis.add_geometry(box)\n#     print(f\"Showing: {ev.name}\")\n#     return False                                  # tell Open3D to redraw\n\n# def next_method(vis):\n#     state[\"idx\"] = (state[\"idx\"] + 1) % len(evaluators)\n#     return refresh(vis)\n\n# key_to_callback = {ord(\"N\"): next_method}  # press ‘N’ to cycle\n# o3d.visualization.draw_geometries_with_key_callbacks(state[\"scene\"], key_to_callback,\n#                                                      window_name=\"Gestalt inspector\")\n\n=======\n\n# from abc import ABC, abstractmethod\n# from typing import Tuple\n#\n#\n# class Evaluator(ABC):\n#     \"\"\"\n#     Any grouping method must override `evaluate`.\n#     It should return:\n#       - groups:   list[list[o3d.TriangleMesh]]\n#       - scores:   list[float]  (one per group, 0-1, or None if not meaningful)\n#     \"\"\"\n#     name: str            # short label used in menus\n#     color: Tuple[float]  # default RGB for this method’s boxes\n#\n#     @abstractmethod\n#     def evaluate(self, cubes) -&gt; tuple[list, list]:\n#         ...\n#\n#\n# class ProximityEval(Evaluator):\n#     name, color = \"Proximity\", (0.2, 0.6, 1.0)  # light-blue\n#\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)\n#         # No per-group quality score → fill with None\n#         return groups, [None]*len(groups)\n#\n#\n# class RegularityEval(Evaluator):\n#     name, color = \"Regularity\", (0.0, 0.8, 0.2)  # green\n#\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)  # reuse\n#         scores = [compute_group_regularity(g) for g in groups]\n#         return groups, scores\n#\n#\n# def groups_to_bboxes(groups, scores, base_color):\n#     boxes = []\n#     for g, s in zip(groups, scores):\n#         aabb = create_group_aabb(g, s if s is not None else 1.0)\n#         # over-write color so each evaluator keeps its own palette\n#         aabb.color = base_color\n#         boxes.append(aabb)\n#     return boxes\n#\n#\n# evaluators = [ProximityEval(), RegularityEval()]\n# state = {\"idx\": 0, \"scene\": scene}  # mutable dict so the lambda sees updates\n#\n# def refresh(vis):\n#     vis.clear_geometries()\n#     vis.add_geometry(*state[\"scene\"])             # raw cubes\n#     ev = evaluators[state[\"idx\"]]\n#     groups, scores = ev.evaluate(state[\"scene\"])\n#     for box in groups_to_bboxes(groups, scores, ev.color):\n#         vis.add_geometry(box)\n#     print(f\"Showing: {ev.name}\")\n#     return False                                  # tell Open3D to redraw\n#\n# def next_method(vis):\n#     state[\"idx\"] = (state[\"idx\"] + 1) % len(evaluators)\n#     return refresh(vis)\n#\n# key_to_callback = {ord(\"N\"): next_method}  # press ‘N’ to cycle\n# o3d.visualization.draw_geometries_with_key_callbacks(state[\"scene\"], key_to_callback,\n#                                                      window_name=\"Gestalt inspector\")\n#\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; '33be097f270828eb783d2d95edd219c6d9c089ea'",
    "crumbs": [
      "Sketching in Gestalt Space"
    ]
  },
  {
    "objectID": "kiosk_demos.html",
    "href": "kiosk_demos.html",
    "title": "Prototype Development",
    "section": "",
    "text": "# #| export\n\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import open3d as o3d\n# import copy\n# import random",
    "crumbs": [
      "Prototype Development"
    ]
  },
  {
    "objectID": "kiosk_demos.html#overview",
    "href": "kiosk_demos.html#overview",
    "title": "Prototype Development",
    "section": "Overview",
    "text": "Overview\nThe paper describes a method for creating abstracted geometry based on an initial pre-segmented input model. This is transformed into an abstracted version by applying Gestalt principles on objects according to sketches by the user. Groupings of the scene elements according to these rules are computed and abstractions are created summarizing the objects into bounding volumes or replacing geometries with scaled versions, depending on the intent of the user abstraction\n\nApplying Gestalt principles to 3D geometry\nDetermining potentials for Abstraction in the scene, based on the Gestalt principles is the most Interesting part of this method.\n\nGenerate scene including a set of clear patterns (grid, dense cluster, line, tight grid, isolated cubes)\nCompute groups based on Gestalt principles (proximity, regularity, continuity, symmetry)",
    "crumbs": [
      "Prototype Development"
    ]
  },
  {
    "objectID": "kiosk_demos.html#implementation",
    "href": "kiosk_demos.html#implementation",
    "title": "Prototype Development",
    "section": "Implementation",
    "text": "Implementation\nIn this notebook I implement a demonstration of the core principles of using gestalt rules to guiding the abstraction of a scene.\nFor this we load / generate a simple scene with primitives in it ordered and aligned in a way so that different patterns and Gestalt shapes emerge naturally to the viewer.\nBased on this the program implements metrics from the paper to programmatically figure out these structures and mark the objects accordingly.\nThe program then visualizes this intial guess and lets the user show his intent of which of the objects to abstract.\nNow using the intent and the previous knowledge a simple abstraction is computed and also rendered in the scene\nThis interactive loop now should already provide a basic understanding of how the method from the paper works albeit being very simple in its design.\n\nDetermining Orientation and Shape Dimensions using PCA\n\n# # Set random seed for reproducibility\n# random.seed(42)\n# np.random.seed(42)\n\n\n# # | export\n# # Generate a simple scene with cube primitives in structured arrangement\n# def generate_cube_grid(n_x=4, n_y=3, spacing=1.5):\n#     cube = o3d.geometry.TriangleMesh.create_box(width=1.0, height=1.0, depth=1.0)\n#     cube.compute_vertex_normals()\n#     scene = []\n#     for i in range(n_x):\n#         for j in range(n_y):\n#             new_cube = copy.deepcopy(cube)\n#             new_cube.translate(np.array([i * spacing, j * spacing, 0]))\n#             scene.append(new_cube)\n#     return scene\n\n\n# # | export\n# def generate_custom_scene():\n#     cube = o3d.geometry.TriangleMesh.create_box(width=1.0, height=1.0, depth=1.0)\n#     cube.compute_vertex_normals()\n#     scene = []\n\n#     def place_grid(start, nx, ny, dx=1.5, dy=1.5):\n#         for i in range(nx):\n#             for j in range(ny):\n#                 c = copy.deepcopy(cube)\n#                 c.translate([start[0] + i * dx, start[1] + j * dy, start[2]])\n#                 scene.append(c)\n\n#     def place_line(start, count, d=1.5):\n#         for i in range(count):\n#             c = copy.deepcopy(cube)\n#             c.translate([start[0] + i * d, start[1], start[2]])\n#             scene.append(c)\n\n#     def place_cluster(center, count=6, spread=0.6):\n#         for _ in range(count):\n#             c = copy.deepcopy(cube)\n#             offset = np.random.normal(scale=spread, size=3)\n#             c.translate(center + offset)\n#             scene.append(c)\n\n#     # Pattern 1: Structured Grid → suggests regularity and proximity\n#     place_grid(start=np.array([0, 0, 0]), nx=3, ny=2)\n\n#     # Pattern 2: Line → suggests continuity\n#     place_line(start=np.array([7, 0, 0]), count=5)\n\n#     # Pattern 3: Dense Cluster → suggests proximity, but low regularity\n#     place_cluster(center=np.array([0, 6, 0]), count=7)\n\n#     # Pattern 4: Small tight grid → proximity + potential symmetry\n#     place_grid(start=np.array([7, 6, 0]), nx=2, ny=2, dx=1.0, dy=1.0)\n\n#     # Pattern 5: Short isolated line → separate group\n#     place_line(start=np.array([3, 3, 0]), count=3)\n\n#     return scene\n\n\n# # | export\n# # Create axis-aligned bounding box for a group of cubes\n# # color them based on their regularity scores\n# def create_group_aabb(group, regularity_score):\n#     all_points = np.vstack([np.asarray(cube.get_axis_aligned_bounding_box().get_box_points()) for cube in group])\n#     aabb = o3d.geometry.AxisAlignedBoundingBox.create_from_points(o3d.utility.Vector3dVector(all_points))\n\n#     # Color code based on score\n#     if regularity_score &gt;= 0.8:\n#         aabb.color = (0.0, 1.0, 0.0)  # Green = regular\n#     elif regularity_score &gt;= 0.5:\n#         aabb.color = (1.0, 0.65, 0.0)  # Orange = moderate\n#     else:\n#         aabb.color = (1.0, 0.0, 0.0)  # Red = irregular\n#     return aabb\n\n\n\nProximity Metric\nThe proximity metric groups objects that are close to one another into a single group\n\n# # | export\n# # Grouping by proximity (Euclidean distance between centers)\n# def group_by_proximity(cubes, threshold=2.0):\n#     centers = [cube.get_center() for cube in cubes]\n#     groups = []\n#     used = set()\n#     for i in range(len(cubes)):\n#         if i in used:\n#             continue\n#         group = [cubes[i]]\n#         used.add(i)\n#         for j in range(i + 1, len(cubes)):\n#             if j not in used and np.linalg.norm(centers[i] - centers[j]) &lt; threshold:\n#                 group.append(cubes[j])\n#                 used.add(j)\n#         groups.append(group)\n#     return groups\n\n\n\nRegularity Scoring\nAs a second metric in combination with proximity i include a regularity score for each group.\nColor bounding boxes based on that score: * 🟢 green = high regularity (≥ 0.8) * 🟠 orange = medium regularity (0.5–0.8) * 🔴 red = low regularity (&lt; 0.5)\n\n# # | export\n# def compute_group_regularity(group):\n#     if len(group) &lt; 3:\n#         return 0.0  # too small to assess regularity\n\n#     centers = np.array([cube.get_center() for cube in group])\n#     diffs = []\n\n#     for i in range(len(centers)):\n#         for j in range(i + 1, len(centers)):\n#             diff = centers[j] - centers[i]\n#             if np.linalg.norm(diff) &gt; 1e-6:\n#                 diffs.append(diff)\n\n#     if len(diffs) &lt; 3:\n#         return 0.0  # not enough meaningful direction vectors\n\n#     diffs = np.array(diffs)\n#     norm_diffs = np.linalg.norm(diffs, axis=1, keepdims=True)\n#     unit_dirs = diffs / norm_diffs\n\n#     # PCA via covariance matrix of unit directions\n#     try:\n#         cov = np.cov(unit_dirs.T)\n#         eigenvalues, _ = np.linalg.eigh(cov)\n#         principal_val = eigenvalues[-1]  # largest eigenvalue = dominant direction\n#         return float(np.clip(principal_val, 0.0, 1.0))  # clip to avoid NaNs\n#     except np.linalg.LinAlgError:\n#         return 0.0\n\n\n# Run demo\n# scene = generate_custom_scene()\n# groups = group_by_proximity(scene, threshold=2.2)\n#\n# bounding_boxes = []\n# for i, group in enumerate(groups):\n#     reg = compute_group_regularity(group)\n#     print(f\"Group {i}: Size={len(group)}, Regularity={reg:.2f}\")\n#     aabb = create_group_aabb(group, reg)\n#     bounding_boxes.append(aabb)\n\nGroup 0: Size=4, Regularity=0.60\nGroup 1: Size=2, Regularity=0.00\nGroup 2: Size=2, Regularity=0.00\nGroup 3: Size=2, Regularity=0.00\nGroup 4: Size=1, Regularity=0.00\nGroup 5: Size=7, Regularity=0.46\nGroup 6: Size=4, Regularity=0.60\nGroup 7: Size=2, Regularity=0.00\nGroup 8: Size=1, Regularity=0.00\n\n\n\n# Visualize\n#o3d.visualization.draw_geometries(scene + bounding_boxes)\n\n\n# from abc import ABC, abstractmethod\n# from typing import Tuple\n\n\n# class Evaluator(ABC):\n#     \"\"\"\n#     Any grouping method must override `evaluate`.\n#     It should return:\n#       - groups:   list[list[o3d.TriangleMesh]]\n#       - scores:   list[float]  (one per group, 0-1, or None if not meaningful)\n#     \"\"\"\n#     name: str            # short label used in menus\n#     color: Tuple[float]  # default RGB for this method’s boxes\n\n#     @abstractmethod\n#     def evaluate(self, cubes) -&gt; tuple[list, list]:\n#         ...\n\n\n# class ProximityEval(Evaluator):\n#     name, color = \"Proximity\", (0.2, 0.6, 1.0)  # light-blue\n\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)\n#         # No per-group quality score → fill with None\n#         return groups, [None]*len(groups)\n\n\n# class RegularityEval(Evaluator):\n#     name, color = \"Regularity\", (0.0, 0.8, 0.2)  # green\n\n#     def evaluate(self, cubes):\n#         groups = group_by_proximity(cubes, threshold=2.2)  # reuse\n#         scores = [compute_group_regularity(g) for g in groups]\n#         return groups, scores\n\n\n# def groups_to_bboxes(groups, scores, base_color):\n#     boxes = []\n#     for g, s in zip(groups, scores):\n#         aabb = create_group_aabb(g, s if s is not None else 1.0)\n#         # over-write color so each evaluator keeps its own palette\n#         aabb.color = base_color\n#         boxes.append(aabb)\n#     return boxes\n\n\n# evaluators = [ProximityEval(), RegularityEval()]\n# state = {\"idx\": 0, \"scene\": scene}  # mutable dict so the lambda sees updates\n\n# def refresh(vis):\n#     vis.clear_geometries()\n#     vis.add_geometry(*state[\"scene\"])             # raw cubes\n#     ev = evaluators[state[\"idx\"]]\n#     groups, scores = ev.evaluate(state[\"scene\"])\n#     for box in groups_to_bboxes(groups, scores, ev.color):\n#         vis.add_geometry(box)\n#     print(f\"Showing: {ev.name}\")\n#     return False                                  # tell Open3D to redraw\n\n# def next_method(vis):\n#     state[\"idx\"] = (state[\"idx\"] + 1) % len(evaluators)\n#     return refresh(vis)\n\n# key_to_callback = {ord(\"N\"): next_method}  # press ‘N’ to cycle\n# o3d.visualization.draw_geometries_with_key_callbacks(state[\"scene\"], key_to_callback,\n#                                                      window_name=\"Gestalt inspector\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[50], line 66\n     63     return refresh(vis)\n     65 key_to_callback = {ord(\"N\"): next_method}  # press ‘N’ to cycle\n---&gt; 66 o3d.visualization.draw_geometries_with_key_callbacks(state[\"scene\"], key_to_callback,\n     67                                                      window_name=\"Gestalt inspector\")\n\nCell In[50], line 63, in next_method(vis)\n     61 def next_method(vis):\n     62     state[\"idx\"] = (state[\"idx\"] + 1) % len(evaluators)\n---&gt; 63     return refresh(vis)\n\nCell In[50], line 53, in refresh(vis)\n     51 def refresh(vis):\n     52     vis.clear_geometries()\n---&gt; 53     vis.add_geometry(*state[\"scene\"])             # raw cubes\n     54     ev = evaluators[state[\"idx\"]]\n     55     groups, scores = ev.evaluate(state[\"scene\"])\n\nTypeError: add_geometry(): incompatible function arguments. The following argument types are supported:\n    1. (self: open3d.cpu.pybind.visualization.Visualizer, geometry: open3d.cpu.pybind.geometry.Geometry, reset_bounding_box: bool = True) -&gt; bool\n\nInvoked with: VisualizerWithKeyCallback with name Gestalt inspector, TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles., TriangleMesh with 8 points and 12 triangles.",
    "crumbs": [
      "Prototype Development"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blenderproc-test-scenes",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "blenderproc-test-scenes",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall blenderproc_test_scenes in Development mode\n# make sure blenderproc_test_scenes package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to blenderproc_test_scenes\n$ nbdev_prepare",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "blenderproc-test-scenes",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/flupppi/blenderproc-test-scenes.git\nor from conda\n$ conda install -c flupppi blenderproc_test_scenes\nor from pypi\n$ pip install blenderproc_test_scenes\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "blenderproc-test-scenes",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "BlenderProc Test Scenes",
    "section": "",
    "text": "Sadly it is not possible to develop a blenderproc pipeline in a notebook fully. Blenderproc needs to execute in a BlenderInternal python interpreter.\nWe can still develop some functionality in notebooks, but only things that can be run without requiring to import the blenderproc libraries.",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#datasets",
    "href": "test.html#datasets",
    "title": "BlenderProc Test Scenes",
    "section": "Datasets",
    "text": "Datasets\nBlenderProc provides easy access to many datasets and freely available assets that can be used for testing using the blenderproc download command.\nOnce you download these assets to your system they can be implemented using different loaders that convert the dataset specific formats into MeshObject instances.\ne.g.: objs = bproc.loader.load_obj(\"mymesh.obj\")",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#manipulating-objects",
    "href": "test.html#manipulating-objects",
    "title": "BlenderProc Test Scenes",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nOnce loaded objects can be manipulated using built in functions. e.g. changing the location, rotation, full pose matrix or transformation matrix.",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#custom-properties",
    "href": "test.html#custom-properties",
    "title": "BlenderProc Test Scenes",
    "section": "Custom properties",
    "text": "Custom properties\nAssigning custom properties might be interesting for semantic segmentation this can be done with the commands obj.set_cp(\"my_prop\", 42) or obj.get_cp(\"my_prop\")\n\nrun blenderproc run .\\quickstart.py\nrun blenderproc vis hdf5 output/0.hdf5\niterate\n\n\n# Visualize the generated images from BlenderProc",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Literature",
    "section": "",
    "text": "SceneNet\n\nFramework for generating high-quality annotated 3D scenes\n\nGoal:\n\nAid Indoor Scene Understanding\nFlexible use for supervised training, 3D reconstruction benchmarks, rendered annotated videos / image sequences.\n\nMethod:\n\nUses manually-annotated datasets of real-world scenes (e.g. NYUv2)\nlearn statistics about object co-occurrences, spatial relationships.\nHierarchical simulated annealing optimisation\nunlimited number of new annotated scenes\nObjects and Textures taken from existing databases\n\nContributions:\n\nDataset with 57 scenes, 5 scene categories\nCreated by human designers and manually annotated at an object instance level.\nMethod to automatically generate new physically realistic scenes.\nScene generation formulated as an optimisation task\nObject relationships, co-occurence and spatial arrangement learned from base scenes and NYUv2 dataset.\nIntroduction of scene variety by sampling objects and textures from libraries.\nWhat is NYUv2?\n\nVideo Sequences from indoor scenes\nRecorded in RGBD using Microsoft Kinect\nPartially labeled dense multi-class labels\nIndoor Segmentation and Support Inference from RGBD Images\nInterpret the major surfaces, objects and support relations of an indoor scene from an RGBD image.\ntypical, messy, indoor scenes\nfloor, walls, supporting surfaces, object regions, recover support relationships\nhow do 3D cues inform a structured 3D interpretation?\nProvides dataset of 464 diverse indoor scenes with detailed annotations.\nImproved object segmentation by being able to infer support structures\n\n\nDifferent Scene Categories: (10 scenes per category, 15-250 objects per scene)\n\nBedrooms\nOffice Scenes\nKitchens\nLiving Rooms\nBathrooms\n\nAutomated Scene Generation\n\nSimulated Annealing\nMan-made scenes as base (SN-BS, NYUv2)\nextract meaningful statistics that allow to generate new configurations of objects\nReplace objects in generated scene with objects of same category with are sampled from databases (ModelNet, Archive3D)\nScene generation inspired by automatic furniture placement methods formulating the problem as an energy optimisation problem.\nA weighted sum of constraints is minimised via simulated annealing:\n\nBounding box intersection: Object bounding boxes should not intersect. Penalise deviation from constraint.\nPairwise distance: Pair together objects that are more likely to co-occur. (Maximum reccomended distance \\(M\\) is a metric in these pairwise constraints)\nVisibility: ensure that one object is fully visible from the other (why is this necessary? Probably so that objects are evenly spread around the room and not clumped in one corner, just like normal interior design would be done)\nDistance to wall: formulate likeliness of objects to be positioned against the wall in distance\nAngle to wall: …and also in angling against the wall\n\nPlug all these constraints into an overall energy function as a weighted sum of all the partial constraint.\nAlgorithm proposes configurations and then optimises these values in an annealing process where the pertubations in orientation and position are decreased in each iteration according to the annealing schedule.\n\nInitialize with all objects centered at the origin.\nEach iteration, variables for randomly selected objects are locally pertubed, until a maximum number of iterations are reached, this accounts as one epoch.\nAfter the epoch check the bounding boxes and visibility constraints, continue next epoch until a feasible configuration is found. (1-3 epochs)\nAfter this Object Placement is finished we get a realisticly cluttered and laid out scene.\n\nTo get even better results object groups are defined and moved together as part of the optimizaion process. This allows each of the grouped objects to be more complex and realistic as if it would be possible with a pure global optimization.\nThere is no limit to the complexity and combination of the layers of groups in the scene that can be generated.\nOne problem with the 3d objects is ensuring that they are all the same relative scale, this is done using an approach by Savva et al.\nEach object comes untextured and is getting applied a texture from a texture library that is appropriate for it. It is uv-mapped automatically in blender. This doesn’t necessarily realistic textures, also as only whole objects and not subparts of objects are textured individually, but it’s main purpose on providing some visual appearance features is still fulfilled.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#scenenet-an-annotated-model-generator-for-indoor-scene-understanding",
    "href": "core.html#scenenet-an-annotated-model-generator-for-indoor-scene-understanding",
    "title": "Literature",
    "section": "",
    "text": "SceneNet\n\nFramework for generating high-quality annotated 3D scenes\n\nGoal:\n\nAid Indoor Scene Understanding\nFlexible use for supervised training, 3D reconstruction benchmarks, rendered annotated videos / image sequences.\n\nMethod:\n\nUses manually-annotated datasets of real-world scenes (e.g. NYUv2)\nlearn statistics about object co-occurrences, spatial relationships.\nHierarchical simulated annealing optimisation\nunlimited number of new annotated scenes\nObjects and Textures taken from existing databases\n\nContributions:\n\nDataset with 57 scenes, 5 scene categories\nCreated by human designers and manually annotated at an object instance level.\nMethod to automatically generate new physically realistic scenes.\nScene generation formulated as an optimisation task\nObject relationships, co-occurence and spatial arrangement learned from base scenes and NYUv2 dataset.\nIntroduction of scene variety by sampling objects and textures from libraries.\nWhat is NYUv2?\n\nVideo Sequences from indoor scenes\nRecorded in RGBD using Microsoft Kinect\nPartially labeled dense multi-class labels\nIndoor Segmentation and Support Inference from RGBD Images\nInterpret the major surfaces, objects and support relations of an indoor scene from an RGBD image.\ntypical, messy, indoor scenes\nfloor, walls, supporting surfaces, object regions, recover support relationships\nhow do 3D cues inform a structured 3D interpretation?\nProvides dataset of 464 diverse indoor scenes with detailed annotations.\nImproved object segmentation by being able to infer support structures\n\n\nDifferent Scene Categories: (10 scenes per category, 15-250 objects per scene)\n\nBedrooms\nOffice Scenes\nKitchens\nLiving Rooms\nBathrooms\n\nAutomated Scene Generation\n\nSimulated Annealing\nMan-made scenes as base (SN-BS, NYUv2)\nextract meaningful statistics that allow to generate new configurations of objects\nReplace objects in generated scene with objects of same category with are sampled from databases (ModelNet, Archive3D)\nScene generation inspired by automatic furniture placement methods formulating the problem as an energy optimisation problem.\nA weighted sum of constraints is minimised via simulated annealing:\n\nBounding box intersection: Object bounding boxes should not intersect. Penalise deviation from constraint.\nPairwise distance: Pair together objects that are more likely to co-occur. (Maximum reccomended distance \\(M\\) is a metric in these pairwise constraints)\nVisibility: ensure that one object is fully visible from the other (why is this necessary? Probably so that objects are evenly spread around the room and not clumped in one corner, just like normal interior design would be done)\nDistance to wall: formulate likeliness of objects to be positioned against the wall in distance\nAngle to wall: …and also in angling against the wall\n\nPlug all these constraints into an overall energy function as a weighted sum of all the partial constraint.\nAlgorithm proposes configurations and then optimises these values in an annealing process where the pertubations in orientation and position are decreased in each iteration according to the annealing schedule.\n\nInitialize with all objects centered at the origin.\nEach iteration, variables for randomly selected objects are locally pertubed, until a maximum number of iterations are reached, this accounts as one epoch.\nAfter the epoch check the bounding boxes and visibility constraints, continue next epoch until a feasible configuration is found. (1-3 epochs)\nAfter this Object Placement is finished we get a realisticly cluttered and laid out scene.\n\nTo get even better results object groups are defined and moved together as part of the optimizaion process. This allows each of the grouped objects to be more complex and realistic as if it would be possible with a pure global optimization.\nThere is no limit to the complexity and combination of the layers of groups in the scene that can be generated.\nOne problem with the 3d objects is ensuring that they are all the same relative scale, this is done using an approach by Savva et al.\nEach object comes untextured and is getting applied a texture from a texture library that is appropriate for it. It is uv-mapped automatically in blender. This doesn’t necessarily realistic textures, also as only whole objects and not subparts of objects are textured individually, but it’s main purpose on providing some visual appearance features is still fulfilled.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation",
    "href": "core.html#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation",
    "title": "Literature",
    "section": "SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?",
    "text": "SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?\nSceneNet RGB-D\n\nDataset\nProvides pixel-perfect ground truth for scene understanding problems (semantic segmentation, instance segmentation, object detection)\ncamera poses, depth data (allows optical flow, camera pose estimation, 3d scene labeling)\nrandom scene layouts, physically simulated object configurations.\n\nGoal:\n\nComparison of semantic segmentation performance of SceneNet vs. VGG-16 ImageNet.\nBoth fine tuned on the same SUN RGB-D and NYUv2 Datasets\nWith Depth data included the performance is even better.\nLarge-scale synthetic datasets with task-specific labels &gt; real-world generic pre-training\n\nWhat is interesting for me here?\n\nThere were some open questions in the original SceneNet paper that could be answered here.\nMore realistic rendering with raytracing\nAddition of Physics engine for object placement instead of just the annealing process.\n\nThe problem of Getting good data\n\nA core need in developing automated methods for scene understanding is having good labeled data with as much information as possible.\nImageNet was a first step in this direction.\nObtaining more Data such as RGB-D data is very hard though if done manually.\nOne step in this direction has been done by sceneNN and scanNet, which use reconstructions from path to get the scene geometry and manually annotated the resulting 3d scenes.\nGetting other and more reliable data is even more complicated from the real world, and requires additional equipment or is not even possible, e.g. wen thinking about dynamic scenes.\nGenerating high quality synthetic data with realistic object placement, rendering, human like camera poses and visual effects has the potential to solve many of these problems effectively\n\nContributions:\n\nVery large dataset with high-quality ray-traced RGB-D images, with lighting effects, motion blur, ground truth labels\nDataset Generation pipeline relying on fully automatic randomised methods wherever possible.\nProposition of an algorithm to generate camera trajectories\nComparison of a Pretrained RGB-CNN from synthetic data with one that is trained on real-world data.\n\nWhat is actually interesting here is that they are using this randomized physics based object placement approach and completey left out the object placement approach that we saw in the normal SceneNet",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-scenenet-main.py",
    "href": "core.html#blenderproc-scenenet-main.py",
    "title": "Literature",
    "section": "BlenderProc SceneNet main.py",
    "text": "BlenderProc SceneNet main.py\nParser:\n\nWe read one scene file using the scene_net_obj_path, which also references the associated .mtl material file.\nThe scene_texture_path defines the folder in which all the textures are stored. These are used to map them to the individual objects corresponding to the object types.\nThe output_dir is just the path to where the generated hdf5 files will be saved\n\nLabel Mapping:\n\nI don’t really understand how the label mappings work. We use a nyu_idset.csv file, that is a internal file from the blenderproc utilities.\nMy guess would be theat the load_scenenet method can somehow extract an identifier from the obj or material file, that using this object type mapping can be use to infer object labels in the custom property category_id.\n\nObjects:\n\nwe use the special load_scenenet method that loads the obj file, and maps the textures from the folder to the object using the previously computed label_mapping.\n\nHandle Floors and Walls:\n\nLook for all walls by filtering the loaded objects by the custom property category_id and looking for the id wall.\nFrom these wall objects we extract floors using a builtin BlenderProc method and rename these newly generated objects as floor.\nWe do the same with the ceilings, with the same builtin method but now looking for the inverse up-vector.\nBoth the newly created floors and ceilings get set a custom property of either “floor” and “ceiling”\n\nHandle Lighting\n\nLamps should emit light, so we look for lights by their name using a regex and add a light surface with a relatively high emission, and having the emission color of the material defined for the lamp.\nAlso the ceilings emit a small bit of light for lighting up the whole room, simulating maybe some light coming in from the windows.\n\nFrom the objects we create a bounding volume hierarchy.\nFinding a camera location:\n\nWe try 10000 time to find 5 poses\nWe sample a location above the floor level, at minimum 1.5 meter and maximum 1.8 meter height.\nWe check that we don’t stand on a object with the camera\nWe find some random orientation for the camera\nWe check that there are no objects directly in front of the camera (1.0 meter)\nWe check that we have a good coverage of the scene with the objects that we are looking at. For doing that we use the bounding volume hierarchy of the scene.\nOnce we passed all these checks we add the resulting pose to the valid cam poses and try again until we found 5 of them.\n\nNow that we have objects loaded, floors and ceilings seperated, and defined good poses for the camera we render the normal maps, depth map, segmentation maps using the custom property category_id\nThen we just render the scene and write the results in the hdf5 format to the output dir.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-scenenetloader.py",
    "href": "core.html#blenderproc-scenenetloader.py",
    "title": "Literature",
    "section": "BlenderProc SceneNetLoader.py",
    "text": "BlenderProc SceneNetLoader.py",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-configuring-the-camera",
    "href": "core.html#blenderproc-configuring-the-camera",
    "title": "Literature",
    "section": "BlenderProc Configuring the Camera",
    "text": "BlenderProc Configuring the Camera\nThe docs talk about how to correctly configure a camera in blenderproc.\nThere are camera intrinsics and extrinsics that you can define. So focal length, optical center vs. position and orientation in the world.\nThe intrinsics are represented by a matrix \\(K = \\begin{pmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1\\end{pmatrix}\\)\nUsing that matrix and the image width and height we can define the full camera intrinsic parameters.\nAlternatively blendproc also can set the camera through blenders own camera parameters, where the focal length or field of view can be defined to set the camera intrinsics.\nNow to get a pose for the camera a transformation matrix can be defined that maps from camera to world coordinate system. This is passed onto the camera as a pose. The camera then when rendering unses these poses and renders an image for each of these.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#the-coco-format",
    "href": "core.html#the-coco-format",
    "title": "Literature",
    "section": "The COCO format",
    "text": "The COCO format\nJSON format, all annotation share the same basic datastructures.\n{\n\"info\": info,\n\"images\": [image],\n\"annotations\": [annotation],\n\"licenses\": [license],\n}",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#more-datasets",
    "href": "core.html#more-datasets",
    "title": "Literature",
    "section": "More Datasets",
    "text": "More Datasets\nThe images from the SceneNet dataset are already very good. But i think that having even a bit more range of what we can use for rendering and other concepts would be good for my toolset.\nThe SunGC Dataset is very promising. Especially sice they also describe how to improve the results by enhancing the quality by exchanging the materials. The paper already interested me anyways so i might as well read it and summarize the most important points here.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#my-takeaways",
    "href": "core.html#my-takeaways",
    "title": "Literature",
    "section": "My takeaways",
    "text": "My takeaways\nLarge scale labelled datasets are important for supervised learning algorithms:\n\nWe dont even want to do supervised learning in any form, but we want labelled datasets of 3d scenes that we can render into rgb-d, semantic segmentation and instance segmentation data.\nInstead of loading arbitrary scenes and inferring object semantics, why not just generate arbitrary scenes that are still realistic.\nOr at least use pre generated scenes for first simple tasks that already contain all the information that we need.\nThis full control over a scene and it’s generation, gives us the power to also apply this to scene abstraction.\nThe concept of there being rules and constrained on how a realistic scene is composed and laid out can be reused when abstracting a scene.\nThe control over the scene generation allows us to replace individual object instances with abstracted object instances and to create a fully abstracted scene from a composition of abstracted objects.\n\nHonestly one bigger question that arises now really is what to do with all that what we know of how BlenderProc works. We have at least some amount of useful tooling now availale. That said here comes then the question of what it really is what we want to achieve at the moment and how to vaidate if we achieved it successfully.\nThis is kind of an open but an important question that my goal is to define today. I just really dont know what i am doing here today. No clue.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#key-information",
    "href": "core.html#key-information",
    "title": "Literature",
    "section": "Key Information",
    "text": "Key Information\n\nWhat is abstract?\n\nMathematical Abstraction (bounding with intervals)\nConceptual Abstraction (represent a family of possible renderings rather than one fixed image)\n\n\n\n\nfoo\n\n foo ()",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "nr_object_stylization.html",
    "href": "nr_object_stylization.html",
    "title": "Non-Realistic Object Stylization",
    "section": "",
    "text": "The paper describes a method for creating stylized geometry for pre-segmented input models. It uses an initial shape analysis step to find the type of fill pattern and its orientation to be used. In an interactive process the user can then choose a fill pattern for each segment. In a clipping stage the fill pattern is then clipped against the model boundary and all segments are assembled into a complete model.\n\n\nThe shape analysis is the most interesting part here, it consists of Random Sample Consensus (RANSAC), which helps to determine the best-fitting primitive shape such as a box-, ellipsoid- or a cone-like structure. It uses a limited amount of vertex samples per iteration to find a primitive shape that is most similar to the shape of the segment.\nPrincipal Component Analysis (PCA) is to determine the axes of maximum variance of the vertex coordinates and computes 3 main axes as well as the ratio of variance between these. This allows to orient the primitives that are fit to the model and help select an appropriate fill pattern.\nWhen both methods are combined good selections for fill patterns can be made that can also be oriented and distributed optimally within the model.\n\n\n\npca-ransac.png\n\n\nSchematic diagram from the paper that shows the outcome of RANSAC and PCA. While the input mesh is given on the left, the center illustrates the shapes detected with RANSAC and the right-hand side points out the main directions of PCA for each segment.",
    "crumbs": [
      "Non-Realistic Object Stylization"
    ]
  },
  {
    "objectID": "nr_object_stylization.html#overview",
    "href": "nr_object_stylization.html#overview",
    "title": "Non-Realistic Object Stylization",
    "section": "",
    "text": "The paper describes a method for creating stylized geometry for pre-segmented input models. It uses an initial shape analysis step to find the type of fill pattern and its orientation to be used. In an interactive process the user can then choose a fill pattern for each segment. In a clipping stage the fill pattern is then clipped against the model boundary and all segments are assembled into a complete model.\n\n\nThe shape analysis is the most interesting part here, it consists of Random Sample Consensus (RANSAC), which helps to determine the best-fitting primitive shape such as a box-, ellipsoid- or a cone-like structure. It uses a limited amount of vertex samples per iteration to find a primitive shape that is most similar to the shape of the segment.\nPrincipal Component Analysis (PCA) is to determine the axes of maximum variance of the vertex coordinates and computes 3 main axes as well as the ratio of variance between these. This allows to orient the primitives that are fit to the model and help select an appropriate fill pattern.\nWhen both methods are combined good selections for fill patterns can be made that can also be oriented and distributed optimally within the model.\n\n\n\npca-ransac.png\n\n\nSchematic diagram from the paper that shows the outcome of RANSAC and PCA. While the input mesh is given on the left, the center illustrates the shapes detected with RANSAC and the right-hand side points out the main directions of PCA for each segment.",
    "crumbs": [
      "Non-Realistic Object Stylization"
    ]
  },
  {
    "objectID": "nr_object_stylization.html#implementation",
    "href": "nr_object_stylization.html#implementation",
    "title": "Non-Realistic Object Stylization",
    "section": "Implementation",
    "text": "Implementation\nIn this notebook I implement a demonstration of each of these methods on an input model and then combine both of them to get similar results to those in the paper.\n\nPrimitive Fitting with RANSAC\nTo implement RANSAC for shape fitting we need to be able to load a model.\nWe also need shapes to fit to the model.\n\n# load model \n\nmodel1 = \"data/armor griffin.glb\"\nmodel2 = \"data/big_soviet_panel_house_lowpoly.glb\"\nmodel3 = \"data/1967 Chevrolet Camaro SS.glb\"\nmodel4 = \"data/1956 BMW Isetta 300 Convertible.glb\"\ndefault = \"Box.glb\"\nscene = trimesh.Scene()\n\nshapes = []\n\nnumber_of_points = 6\ninlier_threshold = 0.1\nnumber_of_trials = 10\n\n#trimesh.util.attach_to_log()\n\nmesh = trimesh.load(default, force='mesh')\nscene.add_geometry(mesh)\n\n'Mesh'\n\n\n\nmesh.show()\n\n\n\n\n\ntype(mesh)\n\ntrimesh.base.Trimesh\n\n\n\nmesh.is_watertight\n\nFalse\n\n\n\nvertices = np.array(mesh.vertices)\nvertices\n\narray([[-0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5],\n       [-0.5,  0.5, -0.5],\n       [ 0.5,  0.5, -0.5],\n       [ 0.5,  0.5,  0.5],\n       [-0.5,  0.5,  0.5],\n       [ 0.5, -0.5,  0.5],\n       [-0.5, -0.5,  0.5],\n       [ 0.5,  0.5, -0.5],\n       [ 0.5,  0.5,  0.5],\n       [ 0.5, -0.5, -0.5],\n       [ 0.5, -0.5,  0.5],\n       [-0.5,  0.5, -0.5],\n       [ 0.5,  0.5, -0.5],\n       [-0.5, -0.5, -0.5],\n       [ 0.5, -0.5, -0.5],\n       [-0.5,  0.5,  0.5],\n       [-0.5,  0.5, -0.5],\n       [-0.5, -0.5,  0.5],\n       [-0.5, -0.5, -0.5],\n       [-0.5, -0.5,  0.5],\n       [-0.5, -0.5, -0.5],\n       [ 0.5, -0.5,  0.5],\n       [ 0.5, -0.5, -0.5]])\n\n\n\n\n\nransac_bounding_box\n\n ransac_bounding_box (points, n_trials=10, sample_size=6, threshold=0.01)\n\n\n\n\nransac_cylinder_with_pca\n\n ransac_cylinder_with_pca (points, n_trials=1000, threshold=0.05)\n\n\n\n\ntrimesh_scene_with_box\n\n trimesh_scene_with_box (mesh, fit_box)\n\n\n\n\ntrimesh_scene_with_sphere\n\n trimesh_scene_with_sphere (mesh, fit_sphere)\n\n\n\n\ntrimesh_scene_with_cylinder\n\n trimesh_scene_with_cylinder (mesh, fit_cylinder, height=None)\n\n\n\n\nrun_ransac_fits\n\n run_ransac_fits (points, threshold=0.01, n_trials=1000)\n\n\n\n\nransac_sphere\n\n ransac_sphere (points, n_trials=1000, sample_size=4, threshold=0.1)\n\n\n# Run RANSAC\nfits = run_ransac_fits(vertices, threshold=0.05, n_trials=500)\nfor name, result in fits.items():\n    print(f\"{name} fit: inliers={result['inliers']} ({(result['inliers']/len(mesh.vertices)*100):.4f}%, fit={result['fit']}\")\n\nbox fit: inliers=24 (100.0000%, fit=(array([-0.5, -0.5, -0.5]), array([0.5, 0.5, 0.5]))\nsphere fit: inliers=24 (100.0000%, fit=(array([-5.20417043e-18, -1.56125113e-17,  0.00000000e+00]), np.float64(0.8660254037844387))\ncylinder fit: inliers=24 (100.0000%, fit={'point_on_axis': array([ 0. , -0.5,  0. ]), 'axis_direction': array([0., 1., 0.]), 'radius': np.float64(0.7071067811865476), 'height': np.float64(1.0)}\n\n\n\ntrimesh_scene_with_box(mesh, fits['box']['fit']).show()\n\n\n\n\n\ntrimesh_scene_with_sphere(mesh, fits['sphere']['fit']).show()\n\n\n\n\n\ntrimesh_scene_with_cylinder(mesh, fits[\"cylinder\"][\"fit\"]).show()\n\n\n\n\n\n\nDetermining Orientation and Shape Dimensions using PCA\n\n\n\nanalyze_pca\n\n analyze_pca (points)\n\n\n# Run PCA\npca_axes, pca_variances = analyze_pca(vertices)\n\nprint(\"PCA Axes:\\n\", pca_axes)\nprint(\"PCA Variance Ratios:\\n\", pca_variances)\n\nPCA Axes:\n [[0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]]\nPCA Variance Ratios:\n [0.33333333 0.33333333 0.33333333]\n\n\n\n\n\nset_axes_equal\n\n set_axes_equal (ax)\n\nSet 3D plot axes to equal scale.\n\n\n\nplot_mesh_with_pca_and_box\n\n plot_mesh_with_pca_and_box (vertices, pca_axes, fit_box)\n\n\nplot_mesh_with_pca_and_box(vertices, pca_axes, fits['box']['fit'])",
    "crumbs": [
      "Non-Realistic Object Stylization"
    ]
  }
]